# Coda Lite Configuration

# Speech-to-Text settings
stt:
  model_size: "base"  # Options: tiny, base, small, medium, large
  language: "en"
  device: "cuda"  # Options: cpu, cuda - Using GPU for faster transcription
  compute_type: "float16"  # Options: float32, float16, int8 - float16 is faster on GPU

# Language Model settings
llm:
  model_name: "llama3"
  temperature: 0.7
  max_tokens: 256
  system_prompt_file: "config/prompts/system.txt"
  tool_prompt_file: "config/prompts/tools.txt"

# Text-to-Speech settings
tts:
  engine: "dia"  # Options: csm, dia
  language: "EN"  # Language code for CSM-1B (EN, ES, FR, ZH, JP, KR)
  voice: "EN-Default"  # Available voices for English: EN-US, EN-BR, EN_INDIA, EN-AU, EN-Default
  speed: 1.0
  device: "cuda"  # Options: cpu, cuda - Using GPU for better performance

  # Dia TTS specific settings
  audio_prompt_path: null  # Path to audio file for voice cloning with Dia TTS
  temperature: 1.3        # Temperature for sampling (higher = more random, lower = more deterministic)
  top_p: 0.95             # Top-p sampling parameter (0-1)
  cfg_scale: 3.0          # Classifier-free guidance scale (higher = more adherence to the prompt)
  use_torch_compile: true  # Whether to use torch.compile for faster inference

# Audio settings
audio:
  input_device: null  # null = default device
  output_device: null  # null = default device
  sample_rate: 16000
  channels: 1

# Memory settings
memory:
  # Short-term memory settings
  max_turns: 20  # Maximum number of conversation turns to store
  max_tokens: 800  # Maximum number of tokens to include in context
  export_dir: "data/memory"  # Directory for memory exports

  # Long-term memory settings
  long_term_enabled: true  # Enable long-term memory
  long_term_path: "data/memory/long_term"  # Path to store long-term memory
  embedding_model: "all-MiniLM-L6-v2"  # Sentence transformer model for embeddings
  vector_db: "chroma"  # Vector database type (chroma, sqlite, in_memory)
  max_memories: 1000  # Maximum number of memories to store
  device: "cpu"  # Device to run embedding model on (cpu or cuda)

  # Memory encoding settings
  chunk_size: 200  # Target size of memory chunks in characters
  chunk_overlap: 50  # Overlap between chunks in characters
  min_chunk_length: 50  # Minimum length of a chunk to be stored

  # Memory persistence settings
  auto_persist: true  # Automatically persist short-term memory to long-term
  persist_interval: 5  # Number of turns between persistence operations

# Personality settings
personality:
  use_advanced: true  # Enable advanced personality features
  lore_file: "config/personality/personal_lore.json"  # Path to personal lore file

# Intent settings
intent:
  enabled: true  # Enable intent routing
  debug_mode: false  # Enable debug mode for intent routing

# Feedback settings
feedback:
  enabled: true  # Enable user feedback hooks
  frequency: 0.3  # Frequency of feedback requests (0.0 to 1.0)
  cooldown: 5  # Minimum number of turns between feedback requests
  apply_adjustments: true  # Apply automatic adjustments based on feedback

# Tool settings
tools:
  enabled: true
  available:
    - get_time
    - tell_joke
    - get_weather
    - get_date
    - get_memory_stats
    - search_memories
    - add_fact
    - add_preference

# Logging settings
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  file: "data/logs/coda.log"
  max_size: 10485760  # 10 MB
  backup_count: 5
